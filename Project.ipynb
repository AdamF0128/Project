{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part to be done in this project is the conversion of provided R Code into Python format. The code was provided by my supervisor and it had to be analysed and rewritten using Python modules, pandas, and functions. The first section deals with the modules added to replicate the R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library(R.matlab) is run as [from scipy.io import loadmat]\n",
    "#library(hyperSpec) done through hyperspy and spectral modules\n",
    "#library(ChemometricsWithR) is obtained through scipy modules and scikit-learn. \n",
    "#library(baseline) is part of scipy module\n",
    "#library(ggplot2) is part of plotnine module\n",
    "#library(pls) is part of sklearn module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import peakutils\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal, misc #Replaces signal library\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "from peakutils import baseline \n",
    "import math\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the main modules, we follow with the loading of the provided Mat format data. This requires converting the files into lists and setting named variables of the files to the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 750  752  754 ... 3996 3998 4000]]\n"
     ]
    }
   ],
   "source": [
    "#Loading wavenumber data\n",
    "wavenumber = loadmat(\"wavenumber.mat\")\n",
    "wavenumber = wavenumber['wavenumber']\n",
    "\n",
    "#LNCaP\n",
    "data = loadmat(\"LNCaP_0Gy.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "LNCaP_0Gy = data['data'] #Extracting the LNCaP spectral data matrix from R Code\n",
    "\n",
    "data = loadmat(\"LNCaP_IF.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "LNCaP_IF = data['data'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savitzky–Golay is then applied to the data, however difficulty is experienced trying to covert the idea of setting the vector to a null object, likely one of the functions is not correct for the type of vector data used. This process was done over the two data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SG Filtering\n",
    "numrow = LNCaP_0Gy.shape[0] # Shape functions as nrow and ncol but specificed to the array elements\n",
    "numcol = LNCaP_0Gy.shape[1]\n",
    "vec1 = {} # None is THE Python equivalent of R's Null object\n",
    "vec1[tuple(np.linspace(1,numrow))] = \"LNCaP 0Gy\" #linspace is seq for use in Python\n",
    "\n",
    "\n",
    "filtLNCaP_0Gy = np.asmatrix(LNCaP_0Gy) # Should work in creating similar matrix\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "for i in range(1,numrow): #Removed {} and got sg filter from python, ordering is n,p,m versus p,n,m\n",
    "\n",
    "  filtLNCaP_0Gy[i,] == savgol_filter(LNCaP_0Gy[i,], 13, 5, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several baseline methods were tested, however none have been able to work so far. There may be an issue based in trying to copy the R code to closely, creating an array that python Baseline functions / methods cannot process. Looking back on the array data to find an issue may be able to get the baseline correction functions to work, however this understanding of the array form will require research as to why the original methods were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Correction Method 1\n",
    "#filtLNCaP_0Gy = filtLNCaP_0Gy[0:10, 0:10]\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#filtLNCaP_0Gy = scaler.fit_transform(filtLNCaP_0Gy.reshape(-1, 1))\n",
    "#LNCaP_0Gy = BaselineRemoval(filtLNCaP_0Gy[0], 1626)\n",
    "#LNCaP_0Gy_corrected = LNCaP_0Gy.IModPoly()\n",
    "#LNCaP_0Gy_corrected.type\n",
    "\n",
    "#for array in np.asarray(filtLNCaP_0Gy):\n",
    "  #LNCaP_0Gy = BaselineRemoval(array, 1626)\n",
    "  #LNCaP_0Gy_corrected = LNCaP_0Gy.IModPoly()\n",
    "  #LNCaP_0Gy_corrected.type\n",
    "    # Takes unknown amount of time to execute (8 hours running did not reach end of execution of code)\n",
    "\n",
    "#Baseline Correction Method 2\n",
    "#LNCaP_0Gy = peakutils.baseline(filtLNCaP_0Gy) \n",
    "  #Creates error 'shapes (4,2738184) and (1684,1626) not aligned: 2738184 (dim 1) != 1684 (dim 0)'\n",
    "\n",
    "\n",
    "#Baseline Correction Method 3\n",
    "#from scipy import sparse\n",
    "#from scipy.sparse.linalg import spsolve  \n",
    "  \n",
    "#def baseline_als(y, lam, p, niter=10):\n",
    "  #L = len(y)\n",
    "  #D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n",
    "  #w = np.ones(L)\n",
    "  #for i in range(niter):\n",
    "    #W = sparse.spdiags(w, 0, L, L)\n",
    "    #Z = W + lam * D.dot(D.transpose())\n",
    "    #z = spsolve(Z, w*y)\n",
    "    #w = p * (y > z) + (1-p) * (y < z)\n",
    "  #return z\n",
    "\n",
    "#LNCaP_0Gy = baseline_als(filtLNCaP_0Gy, 2, 5)\n",
    "  # Creates error 'matrix - rhs dimension mismatch ((1684, 1684) - 1)'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SG Filtering for LNCAP_IF\n",
    "numrow1 = LNCaP_IF.shape[0]\n",
    "numcol1 = LNCaP_IF.shape[1]\n",
    "vec2= {}\n",
    "vec2[tuple(np.linspace(1,numrow1))] = \"LNCaP 2Gy\"\n",
    "\n",
    "filtLNCaP_IF = np.asmatrix(LNCaP_IF)\n",
    "\n",
    "for i in range(1,numrow1):\n",
    "    \n",
    "  filtLNCaP_IF[i,] == savgol_filter(LNCaP_IF[i,],13, 5, 0)\n",
    "\n",
    "#Baseline Correction\n",
    "##LNCaP_IF = baseline(filtLNCaP_IF, 2, hwi=210,  it=5,  int=1050, method='fillPeaks')\n",
    "##LNCaP_IF = LNCaP_IF@corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, Baseline correction has not been able to work with two methods attempted. The issue arrives as there is no Python equivalent to the baseline package in R, which uses 4S Peak Filling – baseline estimation by iterative mean suppression, a paper of which can be found at [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4487348/] however the algorithm is not provided besides the R package that is the basis of the baseline function used in the R code.\n",
    "\n",
    "The savgol filter hasnt been confirmed if it functions as the R code does, but the code that does work appears to run identical (not working code is given ##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.concatenate((vec1, vec2), axis=None).astype(str) #ID = as.character(c(vec1,vec2)), axis must be set to None to create singe dimension vector, then it is set as string type\n",
    "\n",
    "#NORMALIZE spectra (vector normalization)\n",
    "##LNCaP_0Gy = sweep(LNCaP.0Gy, MARGIN=1, apply(LNCaP_0Gy,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n",
    "##LNCaP_IF = sweep(LNCaP.IF, MARGIN=1, apply(LNCaP_IF,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n",
    "\n",
    "#Normalize Spectra Method 1\n",
    "##df2 = np.sqrt(np.sum(np.square(LNCaP_0Gy), axis=0))\n",
    "##LNCaP_0Gy = pd.Series(range(LNCaP_0Gy.shape[0])).apply(lambda row_count: np.divide(LNCaP_0Gy[row_count,:],df2[row_count,]))\n",
    "\n",
    "dataall = np.concatenate([LNCaP_0Gy,LNCaP_IF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating vec1 and vec2 into a string vector was succesful, and similarly done with LNCaP_0Gy and LNCap_IF for datall. The Spectra Normalization was running but when tested, the values did not correpond to the same point as on the R code, even when compensating and not running the baseline code. \n",
    "\n",
    "Trying to fix it led to issues with the array being out of bounds (\"index 1626 is out of bounds for axis 0 with size 1626\")\n",
    "Will attempt to redo the code to figure out how to keep it in bounds\n",
    "\n",
    "The main issue was as with the baseline package, there was no equivalent form of the sweep function in Python. The code used was based on similar attempts, posted online, of replicating the function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-0d2ee95f5730>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-0d2ee95f5730>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    theme_minimal()+\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "##PCA_Data = <- prcomp(dataall ,center = TRUE,scale = FALSE)\n",
    "\n",
    "##fviz_pca_ind(data.pca,label=\"none\",title=\"\",habillage = ID, addEllipses = TRUE, ellipse.level = 0.95)+\n",
    "  #theme_minimal()+\n",
    "  #xlab(\"PC1 (52.9%)\")+\n",
    "  #ylab(\"PC2 (19.3%)\")+\n",
    "  #scale_color_brewer(palette=\"Set1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA analysis was also unable to determine a similar method, and it relied on the data from previous sections, so baseline correction and Normalizing was prioritised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PNT1A\n",
    "\n",
    "###################First two data sets\n",
    "data = loadmat(\"PNT1A_0Gy.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "PNT1A_0Gy = data['data']  #Extracting the PNT1A PNT1A.0Gytral data matrix\n",
    "\n",
    "data = loadmat(\"PNT1A_OF.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "PNT1A_OF = data['data'] \n",
    "\n",
    "#SG Filtering \n",
    "numrow = PNT1A_0Gy.shape[0]\n",
    "numcol = PNT1A_0Gy.shape[1]\n",
    "\n",
    "    filtPNT1A_0Gy = np.asmatrix(PNT1A_0Gy)\n",
    "\n",
    "for i range(1,numrow):\n",
    "    \n",
    "  filtPNT1A_0Gy[i,] == savgol_filter(PNT1A_0Gy[i,],13, 5, 0)\n",
    "\n",
    "#Baseline Correction / basline.fill peaks method 4s Peaks method\n",
    "##PNT1A.0Gy <- baseline(filtPNT1A.0Gy, lambda=2, hwi=210,  it=5,  int=1050,method='fillPeaks')\n",
    "##PNT1A.0Gy <- PNT1A.0Gy@corrected\n",
    "\n",
    "#SG Filtering \n",
    "numrow1 = PNT1A_OF.shape[0]\n",
    "numcol1 = PNT1A_OF.shape[1]\n",
    "\n",
    "filtPNT1A_OF = np.asmatrix(PNT1A_OF)\n",
    "\n",
    "for i range(1,numrow1):\n",
    "    \n",
    "  filtPNT1A_OF[i,] == savgol_filter(PNT1A_OF[i,],13, 5, 0)\n",
    "\n",
    "#Baseline Correction\n",
    "##PNT1A.OF <- baseline(filtPNT1A.OF, lambda=2, hwi=210,  it=5,  int=1050,method='fillPeaks')\n",
    "##PNT1A.OF <- PNT1A.OF@corrected\n",
    "\n",
    "#NORMALIZE spectra (vector normalization)\n",
    "##PNT1A.0Gy=sweep(PNT1A.0Gy, MARGIN=1, apply(PNT1A.0Gy,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n",
    "##PNT1A.OF=sweep(PNT1A.OF, MARGIN=1, apply(PNT1A.OF,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n",
    "\n",
    "#######################Second two data sets\n",
    "data = loadmat(\"PNT1A_0Gy_ICCM.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "PNT1A_0Gy_ICCM = data['data']  #Extracting the spectral data matrix\n",
    "\n",
    "data = loadmat(\"PNT1A_OF_ICCM.mat\") #This loads the matlab file into the workspace as a 'list' object\n",
    "PNT1A_OF_ICCM = data['data'] \n",
    "\n",
    "#SG Filtering\n",
    "numrow = PNT1A_OF_ICCM.shape[0]\n",
    "numcol = PNT1A_OF_ICCM.shape[1]\n",
    "\n",
    "filtPNT1A_0Gy_ICCM = np.asmatrix(PNT1A_0Gy_ICCM)\n",
    "\n",
    "\n",
    "for i range(1,numrow):\n",
    "    \n",
    "  filtPNT1A_0Gy_ICCM[i,] == savgol_filter(PNT1A_0Gy_ICCM[i,],13, 5, 0)\n",
    "\n",
    "#Baseline Correction\n",
    "##PNT1A.0Gy.ICCM <- baseline(filtPNT1A.0Gy.ICCM, lambda=2, hwi=210,  it=5,  int=1050,method='fillPeaks')\n",
    "##PNT1A.0Gy.ICCM <- PNT1A.0Gy.ICCM@corrected\n",
    "\n",
    "#SG Filtering\n",
    "numrow1 = PNT1A_OF_ICCM.shape[0]\n",
    "numcol1 = PNT1A_OF_ICCM.shape[1]\n",
    "\n",
    "filtPNT1A_OF_ICCM = np.asmatrix(PNT1A_OF_ICCM)\n",
    "\n",
    "for i range(1,numrow1):\n",
    "    \n",
    "  filtPNT1A_OF_ICCM[i,] == savgol_filter(PNT1A_OF_ICCM[i,],13, 5, 0)\n",
    "\n",
    "\n",
    "#Baseline Correction \n",
    "##PNT1A.OF.ICCM <- baseline(filtPNT1A.OF.ICCM, lambda=2, hwi=210,  it=5,  int=1050,method='fillPeaks')\n",
    "##PNT1A.OF.ICCM <- PNT1A.OF.ICCM@corrected\n",
    "\n",
    "#NORMALIZE spectra (vector normalization)\n",
    "##PNT1A.0Gy.ICCM=sweep(PNT1A.0Gy.ICCM, MARGIN=1, apply(PNT1A.0Gy.ICCM,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n",
    "##PNT1A.OF.ICCM=sweep(PNT1A.OF.ICCM, MARGIN=1, apply(PNT1A.OF.ICCM,1,function(x) sqrt(sum(x^2))) , FUN=\"/\") #Vector normalisation function(x)sqrt(sum(x^2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is similar to the previous sections, simply using new datasets. The problems and solutions are identical to the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOUR WAY PCA PNT1A in each of 4 classes \n",
    "numrow = PNT1A_0Gy.shape[0]\n",
    "vec1 = {}\n",
    "vec1[tuple(np.linspace(1,numrow))] = \"PNT1A 0Gy\"\n",
    "\n",
    "numrow2 = PNT1A_0Gy_ICCM.shape[0]\n",
    "vec2 = {}\n",
    "vec2[tuple(np.linspace(1,numrow2))] = \"PNT1A 0Gy + ICCM\"\n",
    "\n",
    "numrow3 = PNT1A_OF_shape[0]\n",
    "vec3 = {}\n",
    "vec3[tuple(np.linspace(1,numrow3))] = \"PNT1A OF\"\n",
    "\n",
    "numrow4 = PNT1A_OF_ICCM.shape[0]\n",
    "vec4 = {}\n",
    "vec4[tuple(np.linspace(1,numrow4))] = \"PNT1A OF + ICCM\"\n",
    "\n",
    "\n",
    "dataall = np.concatenate([PNT1A_0Gy,PNT1A_0Gy_ICCM,PNT1A_OF,PNT1A_OF_ICCM])\n",
    "ID = np.concatenate((vec1, vec2, vec3, vec4), axis=None).astype(str)\n",
    "\n",
    "#PCA \n",
    "##data.pca <- prcomp(dataall ,center = TRUE,scale = FALSE)\n",
    "##variance=data.pca$sdev*data.pca$sdev\n",
    "##pvar=variance/sum(variance)\n",
    "##totvar=sum(pvar[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, similar issues as with the above sections. The plotting methods havent been converted, the focus being on the PCA, Baseline Correction, and Normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
